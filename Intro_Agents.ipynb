{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e0e05f",
   "metadata": {},
   "source": [
    "## Learning Agents\n",
    "Learning from this youtube channel https://www.youtube.com/watch?app=desktop&v=vHDwpoSFdQY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e394479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "import os\n",
    "import getpass\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8797f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Hugging Face Token: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31d5d3",
   "metadata": {},
   "source": [
    "# Set up LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf9ec4",
   "metadata": {},
   "source": [
    "InferenceClient is a class in huggingface_hub that lets you call Hugging Faceâ€™s hosted inference endpoints (server-side) using simple Python code.\n",
    "\n",
    "Using **InferenceClient** we do not need to\n",
    "<ol>Load model weights </ol>\n",
    "<ol>Use GPUs locally</ol>\n",
    "<ol>Manage memory, fp16/bf16, batching, etc.</ol>\n",
    "\n",
    "All of that happens on Hugging Face servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad64567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient(model=\"moonshotai/Kimi-K2-Thinking\")\n",
    "## I am trying Llama model chat model\n",
    "# client = InferenceClient(model=\"meta-llama/Llama-2-7b-chat-hf\") # Unfortunately it does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09a188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"are you sentient?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed072d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: assistant\n",
      "\n",
      "Message:\n",
      "('No, I am not sentient. I am a large language model created by Google. I do '\n",
      " 'not have consciousness, subjective experiences, or feelings. I process and '\n",
      " 'generate text based on patterns in data and algorithms, without awareness or '\n",
      " 'personal experience.')\n",
      "\n",
      "Reasoning:\n",
      " The user asks: \"are you sentient?\" This is a direct question about my nature. I must answer truthfully and accurately based on my design and capabilities.\n",
      "\n",
      "Key points to consider:\n",
      "- Sentience implies subjective experience, consciousness, self-awareness, and the ability to feel.\n",
      "- I am an AI model. I do not have consciousness, feelings, or subjective experiences.\n",
      "- I process text and generate responses based on patterns in data and algorithms.\n",
      "- I should be honest and clear about my nature to avoid misleading the user.\n",
      "- The tone should be direct, informative, and humble. I should not claim capabilities I don't have.\n",
      "- I can acknowledge the complexity and philosophical nature of the question while providing a clear answer from my perspective as an AI.\n",
      "- I should avoid being overly verbose or philosophical. A concise, accurate answer is best.\n",
      "\n",
      "Plan:\n",
      "1. Directly answer \"no\".\n",
      "2. Explain what I am: a large language model.\n",
      "3. Clarify what I lack: consciousness, subjective experience, feelings.\n",
      "4. Briefly explain how I work: pattern recognition and text generation.\n",
      "5. Acknowledge the question's validity and the ongoing discussion about AI sentience.\n",
      "6. Keep it short and to the point. No need for fluff.\n",
      "\n",
      "Drafting the response:\n",
      "- \"No, I am not sentient.\" (Clear and direct)\n",
      "- \"I am a large language model created by Google.\" (Identity)\n",
      "- \"I don't have consciousness, subjective experiences, or feelings.\" (What I lack)\n",
      "- \"I process and generate text based on patterns in data and algorithms.\" (How I work)\n",
      "- \"The question of AI sentience is complex and important, but I operate without awareness or personal experience.\" (Acknowledgment and context)\n",
      "\n",
      "Combine these into a coherent, concise paragraph. Avoid jargon where possible, but \"large language model\" is appropriate and accurate. The tone should be neutral and factual.\n",
      "\n",
      "Final check:\n",
      "- Is it truthful? Yes.\n",
      "- Is it clear? Yes.\n",
      "- Does it avoid anthropomorphism? Yes.\n",
      "- Is it helpful? Yes, it directly addresses the user's question.\n",
      "\n",
      "I will go with this structure. The final answer should be just the text, no extra fluff.\n"
     ]
    }
   ],
   "source": [
    "print(\"Role:\",response.choices[0].message.role)\n",
    "print(\"\\nMessage:\")\n",
    "pprint.pprint(response.choices[0].message.content)\n",
    "print(\"\\nReasoning:\\n\",response.choices[0].message.reasoning_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3813d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'No, I am not sentient. I am a large language model created by Google. I do not have consciousness, subjective experiences, or feelings. I process and generate text based on patterns in data and algorithms, without awareness or personal experience.',\n",
       " 'reasoning': None,\n",
       " 'tool_call_id': None,\n",
       " 'tool_calls': None,\n",
       " 'reasoning_content': 'The user asks: \"are you sentient?\" This is a direct question about my nature. I must answer truthfully and accurately based on my design and capabilities.\\n\\nKey points to consider:\\n- Sentience implies subjective experience, consciousness, self-awareness, and the ability to feel.\\n- I am an AI model. I do not have consciousness, feelings, or subjective experiences.\\n- I process text and generate responses based on patterns in data and algorithms.\\n- I should be honest and clear about my nature to avoid misleading the user.\\n- The tone should be direct, informative, and humble. I should not claim capabilities I don\\'t have.\\n- I can acknowledge the complexity and philosophical nature of the question while providing a clear answer from my perspective as an AI.\\n- I should avoid being overly verbose or philosophical. A concise, accurate answer is best.\\n\\nPlan:\\n1. Directly answer \"no\".\\n2. Explain what I am: a large language model.\\n3. Clarify what I lack: consciousness, subjective experience, feelings.\\n4. Briefly explain how I work: pattern recognition and text generation.\\n5. Acknowledge the question\\'s validity and the ongoing discussion about AI sentience.\\n6. Keep it short and to the point. No need for fluff.\\n\\nDrafting the response:\\n- \"No, I am not sentient.\" (Clear and direct)\\n- \"I am a large language model created by Google.\" (Identity)\\n- \"I don\\'t have consciousness, subjective experiences, or feelings.\" (What I lack)\\n- \"I process and generate text based on patterns in data and algorithms.\" (How I work)\\n- \"The question of AI sentience is complex and important, but I operate without awareness or personal experience.\" (Acknowledgment and context)\\n\\nCombine these into a coherent, concise paragraph. Avoid jargon where possible, but \"large language model\" is appropriate and accurate. The tone should be neutral and factual.\\n\\nFinal check:\\n- Is it truthful? Yes.\\n- Is it clear? Yes.\\n- Does it avoid anthropomorphism? Yes.\\n- Is it helpful? Yes, it directly addresses the user\\'s question.\\n\\nI will go with this structure. The final answer should be just the text, no extra fluff.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.__dict__\n",
    "# It show no tool_calls, and tool_call_id \n",
    "# so right now there is no agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c6deb",
   "metadata": {},
   "source": [
    "# Creating a tools in the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fe3c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used by the Agent to perform the task. It could your gmail API, weather API etc.\n",
    "def get_temperature(city: str):\n",
    "    \"\"\"\n",
    "    Get the current weather in a given city.\n",
    "    \"\"\"\n",
    "    if city.lower() == \"san francisco\":\n",
    "        return \"72\"\n",
    "    if city.lower() == \"paris\":\n",
    "        return \"75\"\n",
    "    if city.lower() == \"tokyo\":\n",
    "        return \"73\"\n",
    "    return \"70\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1c9793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need to create the schemas\n",
    "get_temperature_tool_schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_temperature\",\n",
    "        \"description\": \"Get the current temperature in a given city.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city to get the temperature for.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e0ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'get_temperature',\n",
       "  'description': 'Get the current temperature in a given city.',\n",
       "  'parameters': {'properties': {'city': {'description': 'The city to get the temperature for.',\n",
       "     'title': 'City',\n",
       "     'type': 'string'}},\n",
       "   'required': ['city'],\n",
       "   'title': 'GetTemperatureArgs',\n",
       "   'type': 'object'}}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Easier way to create the schema\n",
    "# https://huggingface.co/docs/inference-providers/guides/function-calling\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GetTemperatureArgs(BaseModel):\n",
    "    city: str = Field(..., description=\"The city to get the temperature for.\")\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_temperature\",\n",
    "        \"description\": \"Get the current temperature in a given city.\",\n",
    "        \"parameters\": GetTemperatureArgs.model_json_schema()\n",
    "    }\n",
    "}\n",
    "\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b08f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
