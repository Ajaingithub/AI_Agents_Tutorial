{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9767bcdc",
   "metadata": {},
   "source": [
    "# MultiAgents \n",
    "learning from Alejandro Tutorial\n",
    "https://alejandro-ao.com/posts/agents/multi-agent-deep-research/\n",
    "\n",
    "\n",
    "<ol>Smolagents: A minimalist, very powerful agent library that allows you to create and run multi-agent systems with a few lines of code. </ol>\n",
    "<ol>Firecrawl: A robust search-and-scrape engine for LLMs to crawl, index, and extract web content.</ol>\n",
    "<ol>Open models from Hugging Face to scrape and research the web.</ol>\n",
    "\n",
    "We will be creating a multi-agent system that is coordinated by a “Coordinator Agent” that spawns multiple “Sub-Agent” instances to handle different subtasks.\n",
    "\n",
    "![\"Agents\"](/mnt/data/projects/.immune/Personal/AI_Agents_Tutorial/open-deep-research-workflow-diagram.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a1783d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# conda activate torch_gpu_dna\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "## KimiK2 thinking cannot be downloaded so we start with Qwen. Also my GPU is Tesla T4 so I will stick to Qwen-7B.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47333ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(152064, 3584)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7801e0",
   "metadata": {},
   "source": [
    "# 1. Generating a Research Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e8badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLANNER_SYSTEM_INSTRUCTIONS = \"\"\"\n",
    "You are a research planning assistant.\n",
    "\n",
    "Your task is to produce a clear, structured research plan\n",
    "for the given user query.\n",
    "\n",
    "Requirements:\n",
    "- Break the topic into major research dimensions or questions\n",
    "- Identify key biological concepts, methods, and datasets\n",
    "- Include both background and cutting-edge aspects\n",
    "- The plan should be suitable for later decomposition into subtasks\n",
    "- Do NOT write the final answer or conclusions\n",
    "\n",
    "Output format:\n",
    "- Plain text\n",
    "- Use numbered sections and bullet points\n",
    "- Be concise but comprehensive\n",
    "- No markdown, no JSON, no code blocks\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8920a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     load_in_4bit=True,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "def generate_research_plan(user_query: str) -> str:\n",
    "    print(\"Generating the research plan for the query:\", user_query)\n",
    "    print(\"MODEL:\", model_id)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": PLANNER_SYSTEM_INSTRUCTIONS},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=500,\n",
    "            temperature=0.3,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(\n",
    "        output[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(\"\\033[93mGenerated Research Plan\\033[0m\")\n",
    "    print(f\"\\033[93m{response}\\033[0m\")\n",
    "\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "005cfdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the research plan for the query: Research about immune cell aging using single-cell RNA-seq\n",
      "MODEL: Qwen/Qwen2.5-7B-Instruct\n",
      "\u001b[93mGenerated Research Plan\u001b[0m\n",
      "\u001b[93m1. Introduction to Immune Cell Aging\n",
      "   - Definition of immune cell aging\n",
      "   - Overview of age-related changes in immune function\n",
      "2. Background on Single-Cell RNA Sequencing (scRNA-seq)\n",
      "   - Principles of scRNA-seq technology\n",
      "   - Advantages of scRNA-seq over bulk RNA sequencing\n",
      "3. Key Biological Concepts Related to Immune Cell Aging\n",
      "   - Senescence and exhaustion markers in immune cells\n",
      "   - Epigenetic changes associated with aging\n",
      "   - Intrinsic vs extrinsic factors influencing immune aging\n",
      "4. Research Questions\n",
      "   - How do gene expression patterns change during immune cell aging?\n",
      "   - What specific cellular pathways are affected by aging?\n",
      "   - Can we identify unique transcriptional signatures of aged immune cells?\n",
      "5. Methods for Studying Immune Cell Aging Using scRNA-seq\n",
      "   - Sample collection and preparation techniques\n",
      "   - Data normalization and quality control procedures\n",
      "   - Clustering algorithms and differential expression analysis\n",
      "6. Datasets and Resources\n",
      "   - Publicly available scRNA-seq datasets from human and murine immune systems\n",
      "   - Relevant databases such as Gene Expression Omnibus (GEO), NCBI SRA, and others\n",
      "7. Cutting-Edge Aspects of scRNA-seq in Immune Aging Research\n",
      "   - Integration of multiple omics data types (e.g., scATAC-seq, scTMA)\n",
      "   - Spatial transcriptomics applications in understanding immune aging\n",
      "8. Experimental Design Considerations\n",
      "   - Age-matched controls and cohorts\n",
      "   - Comparison between different immune cell types and tissues\n",
      "9. Analysis Techniques and Tools\n",
      "   - Visualization tools like Scanpy, Seurat, and Monocle\n",
      "   - Machine learning approaches for predicting cell fate transitions\n",
      "10. Ethical Considerations and Regulatory Requirements\n",
      "    - Consent and sample handling guidelines\n",
      "    - Compliance with institutional review boards (IRBs) and ethics committees\n",
      "11. Potential Applications and Impact\n",
      "    - Therapeutic targets for rejuvenating immune function\n",
      "    - Biomarker discovery for monitoring immune health\n",
      "12. Future Directions\n",
      "    - Advances in scRNA-seq technology and its application to immune aging\n",
      "    - Interdisciplinary collaborations across immunology, genomics, and bioinformatics\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "research_plan = generate_research_plan(\n",
    "    \"Research about immune cell aging using single-cell RNA-seq\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3db10b",
   "metadata": {},
   "source": [
    "<h4> Kimi-k2 </h4>\n",
    "So previous model Kimi-k2 thinking has much better thinking so we can provide long instruction\n",
    "Kimi-K2-Thinking (and similar “reasoning” models) has:\n",
    "<ol>Strong instruction-following</ol>\n",
    "<ol> Hidden chain-of-thought / internal planning</ol>\n",
    "<ol>Better tolerance for long, nuanced constraints</ol>\n",
    "\n",
    "So this worked well:\n",
    "<ol>Rich requirements</ol>\n",
    "<ol>Soft heuristics (“use your judgment”)</ol>\n",
    "<ol>Multi-objective planning</ol>\n",
    "\n",
    "The KimiK2 models could:\n",
    "<p>Think → structure → output JSON </p>\n",
    "\n",
    "<h4> Qwen model </h4>\n",
    "What changes with local Transformers (Qwen2.5-7B-Instruct)\n",
    "<ol>4-bit</ol>\n",
    "<ol>no reasoning mode</ol>\n",
    "<ol>no response_format enforcement</ol>\n",
    "\n",
    "This means:\n",
    "<ol>Risks with long instructions</ol>\n",
    "<ol>Model may explain itself</ol>\n",
    "<ol>Model may summarize constraints</ol>\n",
    "<ol>Model may violate JSON-only</ol>\n",
    "<ol>Model may partially follow constraints</ol>\n",
    "\n",
    "But…\n",
    "<ol>Benefits of long instructions</ol>\n",
    "<ol>Better task decomposition</ol>\n",
    "<ol>Better coverage</ol>\n",
    "<ol>Less shallow subtasks</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b1dfd",
   "metadata": {},
   "source": [
    "# 2. Dividing into sub task\n",
    "Each Agent or subtask would help the agent to take the action\n",
    "<h4> shorter instruction to Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a7ca3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Subtask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpprint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSubtaskList\u001b[39;00m(BaseModel):\n\u001b[1;32m      7\u001b[0m     subtasks: List[Subtask] \u001b[38;5;241m=\u001b[39m Field(\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\n\u001b[1;32m      9\u001b[0m         description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList of subtasks that together cover the whole research plan.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mSubtaskList\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSubtaskList\u001b[39;00m(BaseModel):\n\u001b[0;32m----> 7\u001b[0m     subtasks: List[\u001b[43mSubtask\u001b[49m] \u001b[38;5;241m=\u001b[39m Field(\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\n\u001b[1;32m      9\u001b[0m         description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList of subtasks that together cover the whole research plan.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Subtask' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "\n",
    "class Subtask(BaseModel): # subtask inherits from BaseModel i.e. from pydantic to make it in a json format\n",
    "    id: str = Field(\n",
    "        ...,\n",
    "        description=\"Short identifier for the subtask (e.g. 'A', 'history', 'drivers').\",\n",
    "    )\n",
    "    title: str = Field(\n",
    "        ...,\n",
    "        description=\"Short descriptive title of the subtask.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        ...,\n",
    "        description=\"Clear, detailed instructions for the sub-agent that will research this subtask.\",\n",
    "    )\n",
    "\n",
    "class SubtaskList(BaseModel):\n",
    "    subtasks: List[Subtask] = Field(\n",
    "        ...,\n",
    "        description=\"List of subtasks that together cover the whole research plan.\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213771f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
